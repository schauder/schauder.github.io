<p>One of the most difficult tasks in software development is estimating also known as guessing. Here are a couple of thoughts about the topic.</p>
<ol>
<li><strong>Estimates aren't Commitments<&#47;strong>. At least they shouldn't. Lets assume you estimate you need 1 hour from your home to the airport, through all formalities and into the plane. Do you leave your house 1 hour before departure of the plane? Probably not. You'll add all kinds of buffers and leave your house a lot earlier. Same should apply to any kind of project. If you think it takes 6 months it is a bad idea to promise a delivery date in 6 months. Promise 7, 9 or 12 months, depending how bad it is when you show up late.<&#47;li>
<li><strong>Estimates aren't measurments<&#47;strong>. If you want to build a shelf, you can measure the space you have available, and if you measure properly you can cut the boards according to that measurement and they will fit. If they don't it is probably your fault because you didn't measure properly. With the tools in your house you might be able to measure size up to a couple meters with an error of about 1&#47;1000 and if you want to measure more precise you just have to invest in better tools and measure more carefully.Not so with software. If you estimate a task to take a day you might be done after a day. Or maybe after 6 hours. Or you give up after 3 days. There is no way to be sure. You can't just put more effort in estimating to make it more precise. Tools won't help you much. The only thing reducing the margin of error is actually implementing it.<&#47;li>
<li><strong>Some basics in statistics<&#47;strong>. If you roll a dice once you will score approximately 3.5 points. With a margin of error of about 100%. If you roll the dice 1 million times the average value will be 3.5 with pretty good precision. This is no accident, but pretty strong math. The basic idea is that you can add up indipendent random events and that the relative uncertainty is smaller for the added up events than for each single event. This is good news since if you estimate a project you typically break it down into tasks and estimate each task. Then you add up everything and get your result. Which has a smaller range of error.<&#47;li>
<li><strong>The rule above applies does not apply<&#47;strong>. It holds only when the events (guesses&#47;estimates) follow a normal distribution and are independent of each other. Neither is true for software estimates. Go figure.<&#47;li>
<li><strong>We learn through feedback<&#47;strong><br />
It's almost impossible to learn anything without getting feedback on how you are doing. Imagine learning chess without anybody ever telling you when you make a bad or even illegal move. You just have a book with the rules and play games against yourself. How likely do you think it is to ever become a respectable chess player? Nil? I think so too. But with estimating we are often in such a situation. When you do waterfall projects you often do estimates for 100s of tasks in one go and only a year later you get the feedback: The sum of your estimates was 20% of. It's really hard to learn this way. The situation improves a lot when you follow agile practices, where you typically estimate smaller batches, closer to the time when they are actually performed. This enables you to actually learn how long tasks take.<&#47;li><br />
<&#47;ol></p>
